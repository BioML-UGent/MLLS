{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-Lab 7: Linear basis functions and kernels\n",
    "\n",
    "In this PC-lab, we will have a look at two particular strategies in order to be able to analyze non-linear relationships in data: _linear basis functions expansion_ and _kernels_. \n",
    "\n",
    "We will first have a look at basis functions. Next we will turn a logistic regression model to a kernel logistic regression model ourselves. Finally we will study kernel ridge regression on a real-world dataset, where we will predict the compressive strength of concrete based on its composition and age. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: From Linearity to Non-Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine trying to fit a straight line to predict the wage of (commonly educated) people based on their age. This can be a simple linear regression problem, and in many cases, a straight line might do a good job. But what if the relationship isnâ€™t strictly linear? What if the wage suddenly skyrockets after reaching a certain age, or has some other complex, non-linear behavior?\n",
    "\n",
    "This is where linear basis function models come into play. Instead of trying to fit our data using just a line, we transform our input data into a new space, making it easier to fit with a linear model. For example, we might convert our single size feature into multiple features like size, size squared, and size cubed. This way, even though our model is still \"linear\" in its parameters, it can capture non-linear relationships in the original feature space.\n",
    "\n",
    "Now, you might ask: how do we decide on these transformations? How many should we use? Isn't there a smarter way to capture these non-linear relationships without having to manually craft these transformations?\n",
    "\n",
    "Enter kernel methods. Instead of explicitly transforming our data, kernel methods allow us to implicitly operate in a high-dimensional space, enabling us to fit complex relationships without ever leaving the comfort of our linear models. In essence, a kernel is a function that computes the \"similarity\" between data points. This concept unlocks the potential to work with infinite-dimensional spaces and even handle problems where the relationship between inputs and outputs is highly intricate.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/en/c/ce/Cps71_lc_mean.png\" width=400>\n",
    "\n",
    "<em>Example of a fitted kernel regression function, see [KernelRegression](https://en.wikipedia.org/wiki/Kernel_regression) </em>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When trying to solve a binary classification problem, classifiers usually try to separate the positive from the negative class. Often, linear models are not capable of making this separation. An option to deal with this problem is to find an (extended) basis in which the classes are (almost) linearly separable. \n",
    "\n",
    "As an example, consider the binary classification problem in the figure below. The data depicted in this figure originates from a(n) (unknown) joint distribution, for which the Bayes-optimal decision boundary (in case of the 0/1 loss) is a circle. It is clear that the classes are not linearly separable in the original feature space (which is $\\mathbb{R}^2$). An option is to extend this space using a set of (non-) linear functions $$\\phi_1, ... , \\phi_m: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^d. $$ \n",
    "\n",
    "Note that the identity map can also be chosen for $\\phi_i$. Using these transformations, the representation of an instance $\\mathbf{x_i} = (x_{i,1},x_{i,2})$ becomes $$(x_{i,1},x_{i,2}) \\rightarrow \\phi_1(x_{i,1},x_{i,2}),..., \\phi_M(x_{i,1},x_{i,2}) := \\tilde{x}$$,\n",
    "and the model is then fitted on the transformed $\\tilde{x}$.\n",
    "\n",
    "The basis functions are herbey fixed and known. Examples are\n",
    "- $\\phi_j(x_i) = x_i^j \\quad \\forall j = 1, \\dots, m$ (e.g. for polynomial regression)\n",
    "- $\\phi_j(x_i) = \\mathbb{I}(c_j < x_i < c_{j+1}$ (for piecewise constant functions)\n",
    "- $\\phi_j(x_i) = \\exp (- \\frac{x_i - \\mu_j}{2 \\sigma^2})$ (radial basis functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>EXERCISE 7.1 </h2>\n",
    "<p><b>\n",
    "*  Generate a dataset according to the code that is given. Have a look at the documentation \n",
    "<a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_circles.html\" target=\"_top\">here</a>\n",
    "<br>\n",
    "*  Which feature(s) would you construct (using a transformation as described above) in order to separate these two classes? \n",
    "<br>\n",
    "*  Use logistic regression in order to make sure your features are working (make sure you evaluate the performance on a separate test set). In other words, fit a logistic regression model on both the original features and your new features. What's the difference in accuracy between the two? </p> </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. plot the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circles_train, labels_train = make_circles(n_samples=500, noise=0.1, factor=0.6, random_state=23)\n",
    "circles_test, labels_test = make_circles(n_samples=500, noise=0.1, factor=0.6, random_state=85)\n",
    "\n",
    "# visualize points with labl 0 as blue\n",
    "blues = labels_train == 0\n",
    "# visualize points with labl 1 as red\n",
    "reds = labels_train == 1\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "ax.scatter(circles_train[reds,0],circles_train[reds,1], c='r', label=\"class 1\")\n",
    "ax.scatter(circles_train[blues,0],circles_train[blues,1], c='b', label=\"class 0\")\n",
    "ax.set_xlabel(r'$x_1$', size=15)\n",
    "ax.set_ylabel(r'$x_2$', size=15)\n",
    "plt.title('Training data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. What (new) feature makes sense to use as a classification criterion? Construct the new feature and save it as the new train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b: \n",
    "# TODO: construct new features using a transformation of the original features\n",
    "circles_train_new = circles_train[:, 0]**2 + circles_train[:, 1]**2\n",
    "circles_test_new = circles_test[:, 0]**2 + circles_test[:, 1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Use the scikit-learn class [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to fit a Logistic Regression Model on the (original) train data. Make predictions on the test data and calculate the accuracy. \n",
    "For visuaization purposes, evaluate the model on the provided grip points also, and visulize the prediction boundary using the given code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1c: \n",
    "\n",
    "# grid for plotting decision boundary\n",
    "mesh_x1, mesh_x2 = np.meshgrid(np.linspace(-1.5,1.5,100),np.linspace(-1.5,1.5,100))\n",
    "grid = np.asarray([[x1,x2] for x1,x2 in zip(mesh_x1.ravel(), mesh_x2.ravel())])\n",
    "\n",
    "# TODO: Fit logistic regression: \n",
    "# YOUR CODE HERE\n",
    "# compute accuracy\n",
    "acc_before = # YOUR CODE HERE\n",
    "print('Accuracy original features: ' + str(acc_before))\n",
    "\n",
    "# compute predictions for the grid\n",
    "Z = # YOUR CODE HERE (predict on the grid)\n",
    "# reshape to the shape of the grid\n",
    "Z = Z.reshape(mesh_x1.shape)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.contourf(mesh_x1, mesh_x2, Z, cmap=plt.cm.Paired)\n",
    "ax.scatter(circles_train[reds,0],circles_train[reds,1], c='red', label=\"class 1\");\n",
    "ax.scatter(circles_train[blues,0],circles_train[blues,1], c='blue', label=\"class 0\");\n",
    "ax.set_xlabel(r'$X_1$', size=12)\n",
    "ax.set_ylabel(r'$X_2$', size=12)\n",
    "plt.title('Decision boundary on original features')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now do the same for the new train and test data (accuracy and plotting of decision boundary)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Fit logistic regression, calculate accuracy\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "acc_after = # YOUR CODE HERE\n",
    "print('Accuracy new features: ' + str(acc_after))\n",
    "\n",
    "\n",
    "# transform the grid using the same transformation as for the features\n",
    "grid_new = # YOUR CODE HERE\n",
    "# predictions on grid:\n",
    "Z = # YOUR CODE HERE\n",
    "# reshape to the shape of the grid  \n",
    "Z = Z.reshape(mesh_x1.shape)\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.contourf(mesh_x1, mesh_x2, Z, cmap=plt.cm.Paired)\n",
    "ax.scatter(circles_train[reds,0],circles_train[reds,1], c='red', label=\"class 1\");\n",
    "ax.scatter(circles_train[blues,0],circles_train[blues,1], c='blue', label=\"class 0\");\n",
    "ax.set_xlabel(r'$X_1$', size=12)\n",
    "ax.set_ylabel(r'$X_2$', size=12)\n",
    "plt.title('Decision boundary on original features')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having explored **linear basis function models**, we recognized their power in capturing non-linear relationships by transforming our input space into higher dimensions. The crux of such an approach lies in the choice and design of the transformation: how do we choose the right basis functions, and how many of them should we use?\n",
    "\n",
    "Here's where **kernels** present an elegant solution.\n",
    "At their core, kernels offer a clever mathematical trick. Rather than explicitly mapping our input data to a high-dimensional space (as we do with basis functions), kernels allow us to compute the inner products between vectors in this high-dimensional space without ever actually going there.\n",
    "\n",
    "Formally, a kernel is a function $k: \\mathcal{X} \\times \\mathcal{X} \\rightarrow \\mathbb{R}$ defined as $$k(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\langle \\Phi(\\mathbf{x_i}), \\Phi(\\mathbf{x_j}') \\rangle $$\n",
    "Where\n",
    "- $\\boldsymbol{x_i}, \\boldsymbol{x_j}$ are vectors in the input space $\\mathcal{X}$ (ouer feature space, i.e. just pairs of input features)\n",
    "- $ \\langle \\cdot, \\cdot \\rangle $ represents the inner product\n",
    "- $ \\Phi: \\mathcal{X} \\rightarrow \\mathcal{F} $ is a mapping to a feature space $\\mathcal{F}$, analogous to our basis functions\n",
    "\n",
    "\n",
    "This is the brilliance of kernels: they provide an implicit bridge between our input space and a feature-rich space where linear patterns emerge from non-linear relationships.\n",
    "\n",
    "The most simple kernel is the **linear kernel**, for which the kernel function looks as follows: \n",
    "\n",
    "\\begin{equation}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) = \\mathbf{x_i}^T\\mathbf{x_j}. \n",
    "\\end{equation}\n",
    "\n",
    "Consider the following polynomical kernel function given by: \n",
    "\\begin{equation}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) = (\\mathbf{x_i}^T\\mathbf{x_j})^2. \n",
    "\\end{equation}\n",
    "\n",
    "Let again be $\\mathbf{x_i} = (x_{i,1},x_{i,2})$. Then we can give a simple illustration of the kernel trick by writing: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) &= (\\mathbf{x_i}^T\\mathbf{x_j})^2 = (x_{i,1}x_{j,1} + x_{i,2}x_{j,2})^2 \\\\\n",
    "        &= x_{i,1}^2x_{j,1}^2 + x_{i,2}^2x_{j,2}^2 + 2x_{i,1}x_{j,1}x_{i,2}x_{j,2} \\\\\n",
    "        &= (x_{i,1}^2, x_{i,2}^2, \\sqrt{2} x_{i,1}x_{i,2})(x_{j,1}^2, x_{j,2}^2, \\sqrt{2} x_{j,1}x_{j,2}) \\\\\n",
    "        &= \\phi(\\mathbf{x_i})^T\\phi(\\mathbf{x_j}). \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the squared term of the dot product of vectors $\\mathbf{x_i}$ and $\\mathbf{x_j}$ is equivalent to the the product of the explicit feature mapping $\\mathbf{\\phi(x_i)}$ and $\\mathbf{\\phi(x_j)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular kernel is the **Radial Basis Functions (RBF) kernel**. The feature space implied by this kernel is infinite-dimensional: \n",
    "\\begin{align}\n",
    "k(\\mathbf{x_i},\\mathbf{x_j}) = \\exp\\left(- \\frac{||\\mathbf{x_i}-\\mathbf{x_j}||^2}{2\\sigma^{2}} \\right). \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Central to the concept of kernels is the **Kernel Matrix**, often termed the Gram matrix. Having a set of $n$ (train) instances $\\boldsymbol{x_1}, \\dots, \\boldsymbol{x_n}$, it's an $n \\times n$ matrix $K$ where each entry $K_{ij}$ is the kernel function computed between data points $\\boldsymbol{x_i}$ and $\\boldsymbol{x_j}$:\n",
    "\\begin{equation}\n",
    "K = \n",
    "\\begin{bmatrix}\n",
    "    k(\\mathbf{x_1},\\mathbf{x_1}) & ... & k(\\mathbf{x_1},\\mathbf{x_n}) \\\\\n",
    "    \\vdots & & \\vdots \\\\\n",
    "    k(\\mathbf{x_n},\\mathbf{x_1}) & ... & k(\\mathbf{x_n},\\mathbf{x_n}) \\\\\n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "This matrix encapsulates all pairwise similarities between the data points in our dataset. It's pivotal in many kernel-based algorithms since it allows these methods to operate solely using the kernel values, bypassing the need to work explicitly in the high-dimensional feature space. By leveraging this matrix, algorithms like kernel SVM or kernel PCA can find decision boundaries or principal components, respectively, in transformed spaces without ever calculating the transformations directly.\n",
    " Once the model is fit, it can be\n",
    "used to predict the labels of new instances uses a kernel matrix of the following form, for which test instances are denoted with a $*$: \n",
    "\\begin{equation}\n",
    "K^* = \n",
    "\\begin{bmatrix}\n",
    "    k(\\mathbf{x_1}^*,\\mathbf{x_1}) & ... & k(\\mathbf{x_1}^*,\\mathbf{x_n}) \\\\\n",
    "    \\vdots & & \\vdots \\\\\n",
    "    k(\\mathbf{x_L}^*,\\mathbf{x_1}) & ... & k(\\mathbf{x_L}^*,\\mathbf{x_n}) \\\\\n",
    "\\end{bmatrix}, \n",
    "\\end{equation}\n",
    "\n",
    "where we now have $L$ test instances. Using a kernel gives rise to a whole new scale of machine learning methods: \n",
    "- Support Vector Machines (lab in two weeks))\n",
    "- Kernel Principal Component Analysis\n",
    "- Kernel Logistic Regression (this lecture)\n",
    "- Kernel Ridge Regression (this lecture)\n",
    "- ....\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h1>EXERCISE 7.2 </h2>\n",
    "<p><b>\n",
    "* Implement the rbf-kernel and calculate the kernel matrix for the previous dataset. In other words, expand the dataset from the first exercise to the new 'kernel' space. (Tip: You can do this yourself, or use your beloved machine learning library <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.pairwise\" target=\"_top\"> scikit-learn</a>. \n",
    "<br>\n",
    "* Fit a logistic regression model to the expanded features, using the kernel matrix. In this way, we are building our own kernel logistic regression model. What is the performance on the test set?\n",
    "<br>\n",
    "* Which other kernels might work for this dataset? \n",
    " </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the Gaussian (RBF) kernel to compute the gram matrix on your train and test features. Remember that we want to use the pairwise distances from the test set to the train set for the prediction. **TIP**: This simply implies calculating the kernel function with the features as inputs. Use $\\sigma^2=1$, where the bandwidth of the RBF kernel is equivalent to the parameter **gamma** in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "#TODO: calculate the kernel matrix for the training data.\n",
    "K_train = # YOUR CODE HERE\n",
    "# calculate the kernel matrix for the test data\n",
    "K_test = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a logistic regression model to the \"expanded train features\" and evaluate it on the \"expanded test features\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2b): \n",
    "#TODO: Fit a logistic regression model using the kernel matrix: \n",
    "# YOUR CODE HERE\n",
    "# compute predictions\n",
    "predictions_kernel = # YOUR CODE HERE\n",
    "# compute accuracy\n",
    "acc_kernel = # YOUR CODE HERE\n",
    "\n",
    "#print('Accuracy with kernel features: ' + str(acc_kernel))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression: A quick refresher\n",
    "\n",
    "At its core, Ridge Regression is a regularized linear regression. The objective is to not just minimize the residuals, but also to keep the model weights (coefficients) small. As in the lecture:\n",
    "$$\\boldsymbol{w}^{ridge} = \\arg\\min_{\\boldsymbol{w}} \\sum_{i=1}^n (y_i - \\sum_{j=0}^m w_jx_{i1}^j)^2 + \\lambda \\sum_{j=0}^m w_j^2$$\n",
    "where \n",
    "- $\\boldsymbol{w}$ is the weight vector of our model\n",
    "- $\\lambda$ is the regularization parameter.\n",
    "\n",
    "### Extending to Kernel Ridge Regression\n",
    "**Kernel Ridge Regression** marries the principles of Ridge Regression with the power of kernels. Instead of operating in the original feature space or a manually defined transformed space (as with linear basis functions), KRR leverages the kernel trick. KRR optimizes a similar objective as Ridge Regression, but the solution resides in the span of the training data in the feature space defined by the kernel. \n",
    "\n",
    "By making use of the kernel trick, the method will fit a linear model (using an $l_2$-penalty) in the expanded kernel space. For further reading on Kernel Ridge regression see also [WellingKernelRidge](https://web2.qatar.cmu.edu/~gdicaro/10315-Fall19/additional/welling-notes-on-kernel-ridge.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now have a look at the _concreteComprStrength.txt_ dataset. This dataset can be used to predict the compressive strength of concrete based on its composition. Have a look at the file 'concreteComprStrength.info' for more information concerning the variables. The target variable to predict is the 'Concrete compressive strength'.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<h1>EXERCISE 7.2 </h2>\n",
    "<p><b>\n",
    "a) Look at the scikit-learn implementation of ridge regression (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html\" target=\"_top\"> RidgeCV</a>) and kernel ridge regression (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html\" target=\"_top\"> KernelRidge</a>). Use both of them to analyze the `concrete` dataset. In particular,\n",
    "\n",
    "- split your data into train and test (30%) data using the given features and target of the dataframe\n",
    "- standardize your data\n",
    "- fit both Ridge Regression and Kernel Ridge Regression on the train data. For KRR, try both the polynomial kernel and the rbf-kernel. Which one works better?\n",
    "- evaluate the performance on the test data using the root mean squared error (RMSE)\n",
    "  \n",
    "<br> \n",
    "b) KRR can be tweaked in various ways. How many hyperparameters do you have? Choose the polynomial kernel (<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html\" target=\"_top\"> see here for the parameters</a>)  and start doing this by hand to get a feel of the hyperparameters. What's the influence of each hyperparameter? Can you visualize this? \n",
    "<br> \n",
    " </b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load the data in your directory\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/tfmortie/teaching/main/Kernels/concreteComprStrength.txt', 'concreteComprStrength.txt')\n",
    "# read the data into a pandas dataframe\n",
    "df = pd.read_table('concreteComprStrength.txt', delim_whitespace=True, header=0, index_col=None)\n",
    "# shuffle the data\n",
    "df = df.sample(frac=1)\n",
    "# split into features and target\n",
    "features = ['cement', 'blastFurnaceSlag', 'flyAsh', 'water', 'superelastizer', 'coarseAggregate', 'fineAggregate', 'age']\n",
    "target = ['compressiveStrength']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract features and labels from the dataframe, split into train and test set, standardize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO; split the data into training and test set, scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit a Ridge Regression Model to the data, evaluate the RMSE (root mean squared error, square root of MSE) on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "rr = \"..\"\n",
    "\"..\"\n",
    "\n",
    "# compute the MSE\n",
    "mse = \"..\"\n",
    "print('MSE Ridge Regression: ' + str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit a Kernel Ridge Regression Model to the data. Try both the polynomial and the RBF kernel. Again evaluate the MSE on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Analysis for the Polynomial Kernel\n",
    "\n",
    "How many different hyperparameters do we have?\n",
    "\n",
    "The polynomial kernel is given by $$k(\\boldsymbol{x_i}, \\boldsymbol{x_j}) = (\\gamma \\langle \\boldsymbol{x_i}, \\boldsymbol{x_j} \\rangle + c)^d$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: analyze MSE in terms of different hyperparameters.\n",
    "# Either iterate over one hyperparameter at a time or do a heatmap with parameter combinations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.9.1 ('predmod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79f7229f32224f2aa3b7ff71071c711311f5fb22ad26e5b83f3cb875eb6ce551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
