{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC-lab: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Unsupervised learning_ is a different branch of machine learning, as in this case a response variable $y$ is missing. Therefore, unsupervised learning techniques are most often used for exploratory purposes or as a preprocessing step in a supervised context. Unsupervised learning is more prone to subjectivity because results are harder (or even impossible) to validate. This is why one should be careful with the interpretation of results after unsupervised learning. (Those interested can have a look at the paper [\"Clustering: Science or Art\"](http://proceedings.mlr.press/v27/luxburg12a/luxburg12a.pdf), which summarizes a couple of critics and tries to give some pointers considering the evaluation of clustering algorithms). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this PC-lab we will have a look at two frequently applied techniques in the context of unsupervised learning, namely principal component analysis and k-means clustering. We will end with a general scheme, in which both techniques are used. Datasets that will be used in this PC-lab are the `iris`-dataset and `digits`-dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![unsupervised](https://analystprep.com/study-notes/wp-content/uploads/2021/03/Img_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction: Principal Components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gaussianscatterpca](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/800px-GaussianScatterPCA.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular area of unsupervised learning is the area of _Dimensionality Reduction_, in which one tries to reduce the number of variables for visualization purposes or as a preprocessing step for clustering or classification/regression techniques. An established technique which you will find back in most statistics courses is _Principal Components Analysis_ (PCA).\n",
    "\n",
    "Assume a _normalized_ $n\\times p$ data matrix $\\mathbf{X}$. \n",
    "    \n",
    "#### **Goal:** find the direction in $\\mathbf{X}$ with the largest variance (i.e., the most information). \n",
    "\n",
    "In other words, we need to find a linear combination of the inputs:\n",
    "\n",
    "$$ z_{i1} = \\phi_{11}x_{i1}+\\phi_{21}x_{i2}+\\ldots+\\phi_{p1}x_{ip},$$\n",
    "\n",
    ", where $\\mathbf{\\phi}$ is also called the loadings in PCA nomenclature, for which the variance is maximized:\n",
    "\n",
    "$$\\text{maximize}_{\\phi_{11},\\ldots,\\phi_{p1}}\\Big\\{\\frac{1}{n}\\sum_{i=1}^{n}\\Big(\\sum_{j=1}^{p}\\phi_{j1}x_{ij}\\Big)^{2}\\Big\\}\\quad \\text{subject to} \\quad \\sum_{j=1}^{p}\\phi_{j1}^{2}=1.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 1 (warm-up): \n",
    "<br>\n",
    "a) Have a look at the <a href=\"https://en.wikipedia.org/wiki/Iris_flower_data_set\">Iris</a> dataset. Reduce the dataset using PCA and visualize its first two components using a scatterplot (use the original labels in the visualization). Don't forget to preprocess your data. Do you see distinctive groups? \n",
    "<br> \n",
    "b) How much variance is captured in the first three components? \n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_354760/4288135788.py:16: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
      "  plt.style.use('seaborn-white')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from IPython.display import Image\n",
    "\n",
    "plt.style.use('seaborn-white')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing:\n",
    "iris = load_iris()\n",
    "X_train = \n",
    "labels = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##1a): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $K$-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering aims to partition the data in K clusters, so that the within-cluster variation is minimized:\n",
    "\n",
    "$$ \\text{minimize}_{C_{1},\\ldots,C_{K}} \\Big\\{ \\sum_{k=1}^{K}W(C_{k})\\Big\\},$$\n",
    "\n",
    "where the most popular choice for $W(C_{k})$ is the Euclidean distance:\n",
    "\n",
    "$$W(C_{k})=\\frac{1}{|C_{k}|}\\sum_{i,i'\\in C_{k}}\\sum_{j=1}^{p}(x_{ij}-x_{i'j})^{2}.$$\n",
    "\n",
    "K-means clustering uses the following three steps, for which step two and three are repeated until convergence is reached: \n",
    "\n",
    "1) The first step chooses the initial centroids; most easy way of doing this is by choosing K samples at random from the dataset. \n",
    "\n",
    "2) In the second step each element of the dataset is assigned to its nearest centroid. \n",
    "\n",
    "3) New centroids are chosen by taking the mean of all clustered samples according to the previous centroid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 2.1</b>: \n",
    "**Cluster the Iris dataset by means of 2-means, 3-means, and 4-means clustering. Compare the clustering results by visualizing the data in the space induced by the first two principal components.**\n",
    "</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>\n",
    "EXERCISE 3:\n",
    "<br>\n",
    " Calculate the linkage matrix using the \"linkage\" function from \"scipy.cluster.hierarchy\" and visualize it using the \"dendrogram\" function from the same package. Experiment with the different types of linkage and observe how the dendrogram changes. Based on the dendrogram, decide on the number of clusters and cluster the data using the \"AgglomerativeClustering\" function from sklearn. Visualize the results as before.\n",
    "<br>\n",
    "<br>\n",
    "Extra:\n",
    "In practice, when doing exploratory work like this, you probably donâ€™t want to copy-paste or modify the same code repeatedly. Try wrapping this entire procedure in a function to quickly visualize the effect of using a different linkage type and number of clusters. (Plot both the clustering and the dendrogram.)\n",
    "</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Linkage matrix for dendrogram\n",
    "\n",
    "# plot dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform clustering\n",
    "agg_clustering = \n",
    "x_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra: function \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining unsupervised techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often you will find that a number of unsupervised techniques are combined when exploratory analyses are conducted. This is typically the case when your number of variables is high, where you might suffer from the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). In these cases, the approaches laid out above can be combined using the following scheme, which can be tweaked in function of your research question:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Compute the principal components using PCA; \n",
    "\n",
    "2) Select a reduced number of components in function of the explained variance; \n",
    "\n",
    "3) Search for a number of $k$ meaningful clusters; \n",
    "\n",
    "4) Cluster your data using these final settings; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this approach and analyze a more challenging dataset, called the [`digits`-dataset](http://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html). This dataset consists of handwritten images of the numbers 0-9, which has been proprocessed into feature vectors of length 64. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>EXERCISE 4:\n",
    "<br> \n",
    "Apply the approach illustrated above to the `digits` dataset.\n",
    "<br>\n",
    "a) Compute the PCA and create a plot of the explained variance.\n",
    "<br>\n",
    "b) Perform k-means clustering based on different numbers of principal components. Specifically, use a number of PCs that correspond to explaining 20% to 90% of the variance in steps of 10%. (A helper function is provided.) Create one large plot where the different clustering results are shown as subplots. Make sure to label the subplots.\n",
    "<br>\n",
    "c) For the clustering based on 30% of the variance, create a plot where the color represents the clustering and the marker indicates the original label (i.e., the actual number). (You can use the Seaborn library if you prefer!) If this is too easy, try making the markers the actual numbers they represent.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = scale(digits.data)\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_noc_pca(var, threshold):\n",
    "    return np.where(np.cumsum(var)>threshold)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##3a):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1b): \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##1c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 ('predmod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "79f7229f32224f2aa3b7ff71071c711311f5fb22ad26e5b83f3cb875eb6ce551"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
