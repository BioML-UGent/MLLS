{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC Lab 2: Data Preprocessing and Nearest Neighbors\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Nearest neighbor algorithm for classification\n",
    "\n",
    "### Introduction\n",
    "\n",
    "<img src=\"https://www.postnetwork.co/wp-content/uploads/2022/11/irishflowerrs.png\" width=600>\n",
    "\n",
    "### Classification Problem\n",
    "\n",
    "In the previous lab session, the iris flower data set was explored. Imagine now the case that for a \n",
    "new iris flower, we know its respective characteristics (sepal and petal length/height, respectively),\n",
    "but do not know its species. A natural task would be to assign the flower to one of the three possible species, based on its characteristics (features). This task (or problem) is called a \n",
    "<strong> classification problem. </strong> In this practical session, an algorithm to solve this problem by looking at the closest training examples in the (labeled) dataset is described.\n",
    "\n",
    "### Dataset\n",
    "In the iris flower dataset (iris120.csv), each instance (i.e. flower) is described by 5 attributes:\\\n",
    "sepal length, sepal width, petal length, petal width and species.\n",
    "For the matter of simplicity, we will only use the species, sepal length and sepal width. \\\n",
    "\\\n",
    "These properties can be seen as _variables_, and for a given flower, each of these variables takes a specific\n",
    "value. In a classification setting, the aim is to predict the value of one of the variables (here the species), based on the value of the other variables (here sepal width and length). The variable of which the values have to be predicted is called the _output variable_ and the variables used to make this prediction are called the _input variables_ or _features_. \\\n",
    "\\\n",
    "A dataset $\\mathcal{D}$ consists of a set of $n$ observations of input-output couples $(\\boldsymbol{x_i}, y_i)$, where $i \\in \\{1, \\dots, n \\}$. Here, $\\boldsymbol{x_i} = (x_{i1}, \\dots, x_{ip}))^T \\in \\mathbb{R}^p$\n",
    "are the observed values for the features (with $p$ the number of features), and $y_i$ are the observed output values, such that a training dataset $\\mathcal{D}_{train}$ with $n$ instances can be written as\n",
    "$$\\mathcal{D}_{train} = \\{ (\\boldsymbol{x_1}, y_1), \\dots, (\\boldsymbol{x_n}, y_n) \\}. $$ \n",
    "\n",
    "### Model and Problem Setup\n",
    "Using the dataset, the goal is to build a _model_ $f$ that is able to predict the value of the output variable, given the value of the input variables. \\\n",
    "In our iris classification problem, both input variables take real values ($x_i \\in \\mathbb{R}^2$ for $i \\in \\{1, \\dots, n \\}$), whereas the output variable is nominal, taking values from the finite set $\\{setosa, versicolor, virginica\\}$. Hence, the model $f$ we are looking for is a mapping $$f: \\mathbb{R}^2 \\rightarrow \\{setosa, versicolor, virginica\\}.$$\n",
    "\n",
    "### Nearest neighbour classification\n",
    "\n",
    "A very simple technique to derive a classifier model from a given training dataset is the _nearest neighbour algorithm_. It departs from the assumption that instances whose features are highly similar are more likely to have the same label than those with very different features.\\\n",
    "In particular, the _k-nearest neighbors algorithm_ for classification classifies an instance by a plurality vote of its $k$ closest neighbours. If $k=1$, the model applies this idea in its most extreme from: the label is predicted as the label of the closest instance in the training dataset.\\\n",
    "\\\n",
    "In order to select the 'closest' instances in the training set, a suitable measure of distance $d(x_i, x_j)$ between two instances $x_i$ and $x_j$ is used. In our case, we will simply use the Euclidean distance:\n",
    "$$ d_E (x_i, x_j) = \\sqrt{\\sum_{k=1}^p (x_{i,k} - x_{j,k})^2}.$$\n",
    "\n",
    "Using this distance measure, the ($1$-) nearest neighbor algorithm consists of the following steps: \\\n",
    "1. For an instance with unknown label and known feature vector $\\boldsymbol{x}$, calculate the distance to each instance in the dataset: $d_E(\\boldsymbol{x}, \\boldsymbol{x_i})$ where $i = 1, ... ,n.$\n",
    "2. Select the closest instance and take its label as the prediction for the unknown label.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code snippet downloads all necessary files for this pc-lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/BioML-UGent/MLLS/main/02_knn/abalone.csv\",\n",
    "                           \"abalone.csv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/BioML-UGent/MLLS/main/02_knn/iris120.csv\", \"iris120.csv\")\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/BioML-UGent/MLLS/main/02_knn/irisNA.csv\", \"irisNA.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1.1</b>: **Load the dataset iris120.csv in to the memory and select the columns 'Sepal.Length', 'Sepal.Width', and 'Species'. Additionally, load the set of unclassified\n",
    "instances (irisNA.csv) and select the same columns. Both datasets should be loaded as pandas data frames.**\n",
    "</div>\n",
    "\n",
    "Hint: In the following, always replace `\"..\"` with the respective code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "cols = ['Sepal.Length', 'Sepal.Width', 'Species']\n",
    "# load the two datasets and select the respective columns\n",
    "\n",
    "# iris120 as the training set\n",
    "iris120 = \" \" # TODO: load the iris120.csv file into a pandas DataFrame\n",
    "iris120 = \"\" # TODO: select only the columns in the 'cols' list\n",
    "# irisNA as the test set\n",
    "irisNA = \"\" # TODO: load the irisNA.csv file into a pandas DataFrame\n",
    "irisNA = \"\" # TODO: select only the columns in the 'cols' list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris120.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisNA.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 1.2</b>: **Implement the nearest neighbour algorithm (as explained above) for the iris problem in a function called nnIrisPredict.\n",
    "Use this function to predict the species of unknown flowers irisNA.csv in the dataset. Make sure\n",
    "your function has the structure given below. \\\n",
    "(Here, _new_obs_features_ is an array or dataframe containing the two features for one specific new instance, and _train_dataset_ is the dataframe containing the features and labels (i.e. _iris120_ as initialized above)).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_iris_predict(new_obs_features, train_dataset):\n",
    "\n",
    "    # extract features (i.e. first two columns) from training dataset to calculate the distances\n",
    "    train_features = \" \" # TODO\n",
    "\n",
    "    # extract species (labels) of training dataset in a separate variable\n",
    "    train_labels = \" \" # TODO\n",
    "\n",
    "    # create a variable 'dist_euc' which is an array containing the euclidean distance of\n",
    "    # the new instance to all instances (rows) in the training data set\n",
    "    dist_euc = \" \" # TODO\n",
    "\n",
    "\n",
    "    # extract index of nearest neighbor (i.e. the index of the smallest value in the array 'dist_euc')\n",
    "    nn_ind = \" \" # TODO\n",
    "\n",
    "    # extract species label on the respective index\n",
    "    nn_label = \" \" # TODO\n",
    "\n",
    "    return nn_label\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the algorithm to predict the species of the first instance of the unknown flowers dataset irisNA.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features of the first instance in the test set\n",
    "new_obs_features = \" \" # TODO\n",
    "\n",
    "# predict the species of the first instance in the test set\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nearest neighbor algorithm for regression\n",
    "In the previous section, the output $y$ was a nominal variable (i.e. one specific class, discrete set of possible classes). When the output is real-valued ($y \\in \\mathbb{R}$), the prediction problem is called a _regression problem_. \\\n",
    "\\\n",
    "As with nominal outputs, the nearest neighbor algorithm can be also applied in this case. It is identical to the one used in the classification task, where the predicted label now is the real-valued label of the instance closest in the training dataset.\n",
    "\n",
    "### Data preprocessing\n",
    "For solving the next task, some more elementary data preprocessing steps need to be introduced.\n",
    "\n",
    "#### Dummy encoding of nominal variables \n",
    "Often, the features in a dataset are not numerical, but nominal or ordinal (named or named and ordered variables). In this case, to be still able to use algorithms relying on numerical values such as the nearest neighbor algorithm, we can use _dummy encodings_ for each nominal variable.\\\n",
    "\\\n",
    "In dummy encodings, a variable (feature) $x^i$ that can take $k$ possible (nominal) values is replaced by $k$ new binary variables (features). As an example, consider a dataset where one feature (i.e. $x^1$) displays the weather status, taking the $3$ possible values $\\{Sunny, Overcast, Rainy\\}$. Each of these values can be represented by a dummy variable: $x^{1a}$, $x^{1b}$ and $x^{1c}$, with values \n",
    "\n",
    "* $x^{1a} = \\begin{cases} 1,\\quad if \\quad x^1 = \"Sunny\" \\\\\n",
    "                     0, \\quad otherwise \\end{cases}$\n",
    "* $x^{1b} = \\begin{cases} 1,\\quad if \\quad x^1 = \"Overcast\" \\\\\n",
    "                     0, \\quad otherwise \\end{cases}$\n",
    "* $x^{1c} = \\begin{cases} 1,\\quad if \\quad x^1 = \"Rainy\" \\\\\n",
    "                     0, \\quad otherwise \\end{cases}$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we will look at the task of predicting the age of abalone, a type of marine snail. \n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/33/LivingAbalone.JPG/2560px-LivingAbalone.JPG\" width=500>\n",
    "\n",
    "The dataset _abalone.csv_ contains measurements of physical properties of of several abalone specimen. Using these physical properties, the aim is to build a predictive model for the age of these animals (more information concerning this dataset can be found in [abalone.info](https://archive.ics.uci.edu/ml/datasets/Abalone)). In the following example, we replace the nominal variable 'sex' with 3 dummy variables (as many as the values it takes). In python, there are functions such as the [get_dummies()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) function which are used for this purpose. (Another option is scikit-learn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)) So firstly, we create the dummy variables, then we concatenate them with the original dataset and finally we remove the original variable form the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read abalone dataset as pandas dataframe\n",
    "abalone = pd.read_csv(\"abalone.csv\")\n",
    "print(abalone.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables for the \"sex\" feature\n",
    "dummies = pd.get_dummies(abalone.sex)\n",
    "print(dummies.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the dummy variables to the original dataframe\n",
    "abalone_dummy_encoded = pd.concat([abalone, dummies], axis=1)\n",
    "# remove the original 'sex' column\n",
    "abalone_dummy_encoded = abalone_dummy_encoded.drop(['sex'], axis=1)\n",
    "print(abalone_dummy_encoded.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "Missing values are commonly encountered in data mining studies. Often, missing values are imputed\n",
    "(replaced by a value). Several techniques exist to choose this value. A simple, but often used method\n",
    "is mean imputation. Here, each missing value is replaced by the mean of the observed values for\n",
    "that variable. More advanced methods exist of building separate models to predict the missing\n",
    "values.\n",
    "When implementing the mean imputation, the [Imputer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html) of scikit-learn library might be handy. The PC labs in this practical have no missing values however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(abalone_dummy_encoded).any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing the data\n",
    "In realistic datasets, most features have different means and standard deviations. For the nearest\n",
    "neighbour algorithm, it can easily be seen that features with a high standard deviation will be more\n",
    "influential than features with a lower standard deviation. In most cases, this is unwanted since it\n",
    "is not known in advance which features are most important. To overcome this problem, features\n",
    "are often _standardized_, a scaling method where the values are centered around the (sample) mean with a unit standard deviation.\\\n",
    "The standardized version of a feature $x^i$ can be obtained as   \n",
    "$$ x^{i'} = \\frac{x^i - \\mu_i}{\\sigma_i},$$    \n",
    "where $\\mu_i$ and $\\sigma_i$ represent the sample mean and standard deviation of $\\boldsymbol{x^i}$.\n",
    "The [Scaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html) of scikit-learn can be used to perform this standardizing.\n",
    "\n",
    "Notice that what we're doing here (scaling the training and test set together in one operation) is usually considered bad practice as it will leak data from test to train and hence bias model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# split abalone_dummy_encoded into features and labels\n",
    "y = abalone_dummy_encoded['age'].values # keep target variable\n",
    "X = abalone_dummy_encoded.drop(['age'], axis=1) # remove it from the features\n",
    "\n",
    "# scale the features: will return a numpy array\n",
    "X = sklearn.preprocessing.scale(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting and Prediction quality\n",
    "\n",
    "How do we test the performance of a (nearest neighbor) algorithm? \\\n",
    "One obvious idea is tu use a test dataset whith known labels, and compare the predictions made by the algorithm with the true labels. Hence, having our train dataset $\\mathcal{D}_{train}$ and a test dataset $\\mathcal{D}_{test}$, for the regression problem, we can use the _mean of squared residuals_ to eavluate the quality of our model. For any dataset $\\mathcal{D}$, it is calculated as follows:\n",
    "$$ \\text{MSR} = \\frac{1}{|\\mathcal{D} |} \\sum_{\\boldsymbol{x_i} \\in \\mathcal{D}}(f(\\boldsymbol{x_i}) - y_i)^2$$\n",
    " We can compute it for the test set as well as the train set for comparison.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 2.1</b>: **To prepare the dataset for the following exercise, split the dataset in a portion (80%) we will use to train on, and a portion we will use to predict on (20%) (test set). See the documentation of scikit learn's [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for more info on how to do this.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# name the variables for train/test features and labels X_train, y_train, X_test, y_test\n",
    "X_train, X_test, y_train, y_test = # TODO\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-nearest Neighbors for Regression\n",
    "\n",
    "Classifying an instance only by the (one) nearest neighbor to it might not always be wvery accurate. Hence, a simple extension of the neirest neighbor algorithm consists of taking not only one, but multiple neighbors into account. \\\n",
    "\n",
    "To this end, let $N_k(\\boldsymbol{x}) \\subset \\mathcal{D}$ be the $k$ nearest neighbors of an instance with feature vector $\\boldsymbol{x}$.\n",
    "\n",
    "For __classification__:\n",
    "- set $f(\\boldsymbol{x})$ to be the _plurality vote_ of its neighbors, i.e. the label that occures most often in $N_k(\\boldsymbol{x})$\n",
    "\n",
    "For __regression__:\n",
    "- set $f(\\boldsymbol{x})$ to be the _average_ of $N_k(\\boldsymbol{x})$, i.e. $$ f(\\boldsymbol{x}) = \\frac{1}{k} \\sum_{x_i \\in N_k(x)} y_i.$$\n",
    "\n",
    "### K-nearest neighbors in Python\n",
    "\n",
    "As with the $1$-nearest neighbour algorithm, it is possible to implement a version of the k-nearest neighbours\n",
    "algorithm in python from scratch. However, to simplify things, an alternative is to use a pre-implemented version of the algorithm.\\\n",
    "HEre, we use the  [scikit-learn](http://scikit-learn.org/stable/) library again, which has an implementation of \n",
    "the $k$-nearest neighbor algorithm availbale. You can load and see more info about the usage of this function by going to the [documentation page](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)\n",
    "or else by typing the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "help(neighbors.KNeighborsRegressor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the help window, in scikit-learn, an estimator for classification/regression is a _Python object_ that implements the methods _fit(X, y)_ and _predict(T)_.\\\n",
    "\\\n",
    " The constructor of an estimator takes as arguments the parameters of the model (in our case the basic parameters are the number of neighbours and the distance metric). So the first step is to initialize this constructor. Here, we set the number of \n",
    " neighbors to be considered to be $k=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 5\n",
    "knn = neighbors.KNeighborsRegressor(n_neighbors=n_neighbors, metric='euclidean')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call our estimator instance `knn`. It now must be fitted to the data, that is, it must learn from the data. This is done by passing our training set to the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the estimator is fitted to the training data, you can ask to the estimator which is the age of the first example in our test dataset (remember: by doing this, we are now comparing the features of this example to the features of all training samples, then determining what the closest neighbors are, then averaging the age of those neighbors as predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = knn.predict(X_test[0].reshape(1,-1))\n",
    "y_pred_1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the mean squared error of the prediction we got before (for the first example of the test dataset) by calculating it ourselves or by using the [mean_squared_error()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) function of scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "print(y_test[0], y_pred_1)\n",
    "print(mean_squared_error(y_pred_1, y_test[0].reshape(1, -1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3.1</b>: **Build a 3-nearest-neighbor regressor in a similar manner as before. Predict the age for all training samples and test samples and compute the mean squared error of both prediction sets. Which predictions are better?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now use k=3 neighbors for training\n",
    "n_neighbors = 3\n",
    "# Instantiate the knn object\n",
    "knn = # TODO\n",
    "# Fit\n",
    "# TODO\n",
    "# Predict train\n",
    "y_pred_train = # TODO\n",
    "# Predict test\n",
    "y_pred_test = # TODO\n",
    "# print MSE train\n",
    "print(mean_squared_error(y_pred_train, y_train))\n",
    "# print MSE test\n",
    "print(mean_squared_error(y_pred_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE 3.2</b>: **Now do the same but iterate over possible values for 'k' as the number of nearest neighbors taken into account, where k ranges between 1 and 50 (step size equal to 3). Plot the resulting errors of the models as a function of 'k'. 'k' is what we call a hyperparameter: a type of parameter that is (most often) tuned by hand according to the prediction task. For some datasets/tasks we will for example need more or less neighbors (as predictions tasks have varying difficulty).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use lists to store the results obtained from every iteration\n",
    "results_train = []\n",
    "results_test = [] \n",
    "for i in range(1, 50, 3):\n",
    "    # instantiate the model\n",
    "    \"..\" # TODO\n",
    "    # Fit to train data\n",
    "    \"..\" # TODO\n",
    "    # Predict train\n",
    "    y_pred_train = \"..\" # TODO \n",
    "    # Predict test\n",
    "    y_pred_test = \"..\" # TODO\n",
    "    # calculate and store the MSE train\n",
    "    mse_train = \"..\" # TODO\n",
    "    \"..\" # TODO append error in results list\n",
    "    # calculate and store MSE test\n",
    "    mse_test = \"..\" # TODO\n",
    "    \"..\" # TODO append error in results list\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, 50, 3), results_train) # training MSEs\n",
    "plt.plot(range(1, 50, 3), results_test) # test MSEs\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('$k$ nearest neighbors')\n",
    "plt.legend(['Train MSE', 'Test MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distuq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
