{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yqd-E7gdqZBs"
      },
      "source": [
        "# PC lab 3: Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqz2SRIbqZBx"
      },
      "source": [
        "## 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orkPp1-uqZBx"
      },
      "source": [
        "The goal of linear regression is to model the relationship between one or more features *x* and a **continuous** variable *y*. When there is only one feature $x_1$ (univariate linear regression), the equation of the regression line is:\n",
        "\n",
        "<center>$\\hat{y} = w_{0} + w_{1}x_{1}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0ANrT8zqZBy"
      },
      "source": [
        "where $w_{0}$ is the intercept on the y-axis, or bias. In the case of multiple features, we talk about multiple linear regression, and y is modelled as a linear combination of the features, weighted by some set of weights w:\n",
        "\n",
        "<center>$\\hat{y} = w_{0}x_{0} + w_{1}x_{1} + ... + w_{p}x_{p} = \\sum\\limits_{i=0}^{p}w_{i}x_{i}$,\n",
        "\n",
        "<p align=\"justify\">In this notation, we introduced an additional feature $x_{0}$ which always equals 1. This notation allows to formulate linear regression as a matrix multiplication:\n",
        "\n",
        "<center>$\\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{w}$,\n",
        "\n",
        "<p align=\"justify\">where $\\mathbf{\\hat{y}}$ (a vector of dimension $n \\times 1$) contains the predicted target variable for the $n$ instances, $\\mathbf{w}$ is a $(p+1) \\times 1$ vector containing the weights and $\\mathbf{X}$ is a $n \\times (p+1)$ matrix with the features. Note that the first column of $\\mathbf{X}$ is a column of ones, because it represents the feature $x_{0}$ that we introduced to go with the intercept. We will stick to this matrix notation for the rest of this lab, as this notation is common practice in machine learning. Also, this notation makes it much easier to translate an algorithm into python code. Make sure that you understand it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKN22v6EqZBy"
      },
      "source": [
        "Fitting this model to a set of data comes down to finding the weight vector $\\mathbf{w}$ that minimizes the discrepancy between the true target values $\\mathbf{y}$ and the predictions $\\hat{\\mathbf{y}}$. As is often the case in regression problems, this discrepancy between true values and predicted values is expressed by the total sum of squared errors or the **residual sum of squares (RSS)**:\n",
        "\n",
        "<center>$RSS = \\sum\\limits_{i=1}^{n}(y_{i} - \\hat{y}_{i})^2$,\n",
        "\n",
        "or, equivalently:\n",
        "\n",
        "<center>$RSS = (\\mathbf{y} - \\mathbf{Xw})^{T}(\\mathbf{y} - \\mathbf{Xw})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PQErPaZqZBz"
      },
      "source": [
        "Note that the mean sum of squares (MSE) is perfectly equivalent to the RSS.\n",
        "\n",
        "\n",
        "For most machine learning algorithms we need an optimization algorithm such as gradient descent to find the set of weights that minimize the discrepancy between true and predicted values. However, for linear regression there is a convenient analytical solution to find the optimal weight vector $\\mathbf{w}$ that minimizes the RSS. This solution is obtained by solving the so-called normal equations, leading to the following expression:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0e0zykJqZBz"
      },
      "source": [
        "<center>$\\mathbf{w_{OLS}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jA1ILGreqZB0"
      },
      "source": [
        "This solution is called the ordinary least squares or OLS solution. With this equation in our toolbox, we can fit a linear regression model to a toy dataset. Let's first simulate some datapoints from a linear ground truth, but with some Gaussian noise $\\epsilon$ added to the observations. We will use only one feature $x$ so that we can visualize the regression line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBIDPn2CqZB0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "x = np.random.randn(100) # sample of a hundred random gaussian points\n",
        "y = x + np.random.randn(100)/2 # simulating an output: y = X + some noise\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(x,y)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JjMNziEqZB2"
      },
      "source": [
        "Now, let's use linear regression to model $y$ as a function of $x$. We can use the analytic result from the normal equations to find the optimal weight vector $\\mathbf{w_{OLS}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}$. NumPy makes easy work of this.\n",
        "\n",
        "First, let's create a feature matrix with an intercept. (Remember, we are fitting $\\hat{y} = w_{0} + w_{1}x_{1}$, which is equivalent to having $\\hat{y} = w_{0}*1 + w_{1}x_{1}$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zp-qRbgdqZB3"
      },
      "outputs": [],
      "source": [
        "# Put the features in a matrix, with a column of ones for the intercept\n",
        "X = np.ones((100,2))\n",
        "X[:,1] = x\n",
        "\n",
        "print(X.shape)\n",
        "print(X[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtSx93SZAeeZ"
      },
      "source": [
        "Now we can estimate the weights of the linear model via: $\\mathbf{w_{OLS}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{y}$.\n",
        "Using NumPy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tp3tAK8_59n"
      },
      "outputs": [],
      "source": [
        "w = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)),X.T),y)\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCgqSyyPqZB4"
      },
      "source": [
        "We find the weight vector that minimizes the MSE between observations and predictions. With this weight matrix, we have succesfully \"trained\" a predictive model. We can now use this model to make predictions. Let's say we have a new data point that we do not know the output for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGwc3cqCqZB5"
      },
      "outputs": [],
      "source": [
        "X_new = np.array([1, np.random.randn()])\n",
        "X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eiU1bFp-Bclq"
      },
      "outputs": [],
      "source": [
        "y_pred = np.matmul(X_new, w)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wd7pq78BqLh"
      },
      "source": [
        "**Remember**: the primary goal of machine learning is to make predictions for new data as well as possible. This contrasts with the goal in statistics, where one would - typically - derive p-values of the estimated parameters at this point, to infer if the data supports evidence for there being a trend.\n",
        "\n",
        "This goal makes us care less about underlying assumptions. If, for a statistician, an assumption is violated, the resulting estimated p-value may reflect false associations, and the study is invalidated. For a ML practitioner, violated assumptions typically just mean our model could be made better still."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhTJpIbQqZB5"
      },
      "source": [
        "To evaluate a Machine learning model, we need data separate from the data we trained on (after all, we are interested in knowing how good the model is on new data). In machine learning terms, we call this a \"test set\". Let's simulate one and make predictions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGsAPhAKEEYZ"
      },
      "outputs": [],
      "source": [
        "X_test = np.array([[1] * 100, np.random.randn(100)]).T\n",
        "y_test = X_test[:,1] + np.random.randn(100)/2\n",
        "\n",
        "y_hat = np.matmul(X_test, w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfREh2yPqZB5"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X_test[:,1],y_test)\n",
        "ax.set_xlabel('x')\n",
        "ax.set_ylabel('y')\n",
        "ax.plot(X_test[:,1], y_hat, color='orange')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcZLn-tbqIEd"
      },
      "source": [
        "We can evaluate the performance of our model by computing the MSE:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRyX92KAqIil"
      },
      "outputs": [],
      "source": [
        "# Function to compute MSE\n",
        "def compute_MSE(y_true, y_predicted):\n",
        "    \"\"\"Obtain MSE between true y's and predicted y's\"\"\"\n",
        "    return np.mean((y_true-y_predicted)**2)\n",
        "\n",
        "MSE = compute_MSE(y_test, y_hat)\n",
        "print('Mean squared error: {}'.format(MSE))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT7QJztuFEbf"
      },
      "source": [
        "Alternatively, we can evaluate by using the mean absolute error (MAE):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPrJX4cRFIkn"
      },
      "outputs": [],
      "source": [
        "# Function to compute MAE\n",
        "def compute_MAE(y_true, y_predicted):\n",
        "    \"\"\"Obtain MAE between true y's and predicted y's\"\"\"\n",
        "    return np.mean(np.abs(y_true-y_predicted))\n",
        "\n",
        "MAE = compute_MAE(y_test, y_hat)\n",
        "print('Mean absolute error: {}'.format(MAE))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9AHHlp-qZB7"
      },
      "source": [
        "## 2 Application: predicting house prices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBmNcQxJqZB7"
      },
      "source": [
        "We will use a [housing dataset](https://www.kaggle.com/harlfoxem/housesalesprediction) hosted on Kaggle for a practical illustration, slightly modified for the purpose of this PC lab. The dataset contains the price of 21613 houses in US dollars, together with the following features:\n",
        "1. **categorical features** (discrete categories): waterfront yes/no, color (yellow, blue, white or 'other')\n",
        "2. **ordinal features** (take on discrete values, but there is a natural ordering, unlike categorical features): number of bedrooms, bathrooms, floors, yr_built\n",
        "3. **continuous features**: surface areas: sqft_living, sqft_lot, sqft_above, sqft_basement, and latitude, longitude\n",
        "\n",
        "We would like to build a model that can predict the price of a house based on these features. First, let's read in the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIPobbwhqZB7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv('https://raw.githubusercontent.com/BioML-UGent/MLLS/main/03_linear_regression/pc3_housingdata_modified.csv')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY6JKwtKqZB7"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtUWYyNFqZB7"
      },
      "source": [
        "Let's visualize some features to get an idea of what this data looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xdPEC6fqZB7"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(data.loc[:,['price', 'sqft_living', 'sqft_basement', 'lat', 'long']], plot_kws = {'alpha': 0.3, 'linewidth' : 0, \"s\": 5});"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sFT5T3cqZB8"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data.loc[:,['price','color', 'waterfront', 'bathrooms', 'bedrooms']], plot_kws = {'alpha': 0.3, 'linewidth' : 0, \"s\": 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-5C6BiuqZB8"
      },
      "source": [
        "There are different types of features here. How can we use the color attribute in a linear regression model? In general, dummy variables are introduced to encode categorical features. In machine learning, this is more often called **one-hot encoding**. It turns out that both [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and [pandas](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) have functions that can do this trick. Let's use pandas, since it can handle text features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGC-lxuvqZB8"
      },
      "outputs": [],
      "source": [
        "dummies = pd.get_dummies(data.color)\n",
        "dummies.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_e1BgNzLqZB9"
      },
      "source": [
        "Let's add these encoded features to the dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Z5aGwuTqZB9"
      },
      "outputs": [],
      "source": [
        "data_onehot = pd.concat([data, dummies], axis=1)\n",
        "data_onehot = data_onehot.drop(['color'], axis=1) # remove the original color column\n",
        "data_onehot.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDkhvS4GqZB9"
      },
      "source": [
        "We can use these dummy variables in a linear regression model. First, it's good to extract the data from the pandas dataframe into numpy arrays, since most machine learning APIs are compatible with numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3r6Yq8dqZB9"
      },
      "outputs": [],
      "source": [
        "# Put data in numpy arrays\n",
        "y = data_onehot.price.values\n",
        "X = data_onehot.drop('price', axis=1).values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_2zvs4uqZB9"
      },
      "source": [
        "Finally, we would like to evaluate the performance of our data on new, unseen data, that was not used to train the model. Therefore, we split up our dataset into a training and a test dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_pDKvBhqZB9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7) # Use 70% of data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf2PFcEPJ0NV"
      },
      "source": [
        "To fit any type of ML model, we recommend the industry-standard sklearn library.\n",
        "Take a look at the documentation for its [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) model to learn how to apply it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFqn8zB-dgff"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE:</b>\n",
        "<p> Using the documentation linked above, complete the code below to train a linear regression model on the training set and then evaluate it on the test set.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZZMQMPFqZB-"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "LinReg = ... # ADD CODE call an instance of the class LinearRegression\n",
        "LinReg.fit(...) # ADD CODE fit the model on the training data\n",
        "\n",
        "y_pred = LinReg.predict(...) # ADD CODE make predictions for the test data\n",
        "MAE = compute_MAE(y_test, y_pred)\n",
        "print('Test set MAE: {}'.format(MAE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ptfam1f6th"
      },
      "source": [
        "Creating the model and evaluating it is typically only the first step.\n",
        "Let's dig in some more to the model to know if some stuff is going wrong.\n",
        "\n",
        "First, let's see if we are **overfitting** the model. An overfit means that the model is learning patterns on the training set that are not directly applicable to the test set. This is the case if the error that the model makes on the training set is a lot lower than on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H556tlsYgcri"
      },
      "outputs": [],
      "source": [
        "y_pred = LinReg.predict(X_train) # make predictions for the test data\n",
        "\n",
        "MAE = compute_MAE(y_train, y_pred) # compute the MAE\n",
        "print('Train set MAE: {}'.format(MAE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP5rYTw5gjD5"
      },
      "source": [
        "The error you see should be about as big on the train set than it is on the test set. This is good news, our model is not to overly complex. Still, how happy are we with our model? Let's evaluate with another score. The **R2 score** evaluates the proportion of variability in the output is captured by the model. A score of 1 means all variability in house prices is captured (perfect predictions), a score of 0 means our model is no better than just guessing the mean house price."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8tjBsXMgvZQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred = LinReg.predict(X_test) # make predictions for the test data\n",
        "\n",
        "r2 = r2_score(y_test, y_pred) # compute the MAE\n",
        "print('Test set r2 score: {}'.format(r2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpSOaI-vhIUg"
      },
      "source": [
        "Our model explains ~60-65 percent of variability in house prices. Quite okay, but can probably be improved. Before we try do so, let's look a bit deeper in the model.\n",
        "\n",
        "A question one might ask is, if the model is making bad predictions for one type of house price (e.g. specifically the more expensive houses). Let us investigate by making a **predicted vs actual plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNBaqPNSiIEH"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test,y_pred, s = 5, alpha = 0.3);\n",
        "ax.set_xlabel('y_test (Actual)');\n",
        "ax.set_ylabel('y_pred (Predicted)');\n",
        "ax.plot(np.arange(y_test.min()-1e5, y_test.max()+1e5), np.arange(y_test.min()-1e5, y_test.max()+1e5), color='orange');\n",
        "plt.xlim([y_test.min()-1e5, y_test.max()+1e5]);\n",
        "plt.ylim([y_test.min()-1e5, y_test.max()+1e5]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uzfQJspjTNd"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>THOUGHT EXERCISE:</b>\n",
        "<p> Interpret the plot, what is going wrong with the model?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5hbCmpwjc9V"
      },
      "source": [
        "As a last step, let's check the coefficients of the model to see if everything makes sense to us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLrlSj2YjhDs"
      },
      "outputs": [],
      "source": [
        "coefficients = LinReg.coef_\n",
        "feature_names = data_onehot.drop(\"price\", axis =1).columns.values\n",
        "for name, coef in zip(feature_names, coefficients):\n",
        "    print(name, \"\\t\", coef)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwoMLR7ekAh8"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>THOUGHT EXERCISE:</b>\n",
        "<p> Interpret the learned weights. Do they make sense? Does it even make sense to learn a linear relation between all of these features and house prices? Can you come up with a more appropriate solution to learn the impact of: (1) latitude and longitude?, (2) correlated features such as numbers of bedrooms and bathrooms? (3) differing feature numerical ranges?</p>\n",
        "</div>\n",
        "\n",
        "In the remainder of the PC lab, we will tackle the issue of learning non-linear effects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bhxcwU2mW0F"
      },
      "source": [
        "## 3 Expanding the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiS6cHg9oAk_"
      },
      "source": [
        "### 3.1 Polynomial feature expansion: Interaction and non-linear effects\n",
        "\n",
        "We can model non-linear relations by performing some non-linear transformation $\\phi(\\mathbf{x})$ on the original features $\\mathbf{x}$. This is known as a **basis function expansion**. An example of such a transformation are polynomial basis functions, where we consider higher-order powers of the original features:\n",
        "\n",
        "$\\phi(x) = [1, x, x^2, x^3,...,x^d]$\n",
        "\n",
        "It is important to note here that, although we use non-linear transformations, the model linear regression model $y = f(\\phi(x)$ will still be linear in the parameters:\n",
        "\n",
        "$\\hat{y} = w_{10}x_{0} + w_{11}x_{1} + w_{12}x_{1}^2 ... + w_{md}x_{m}^d$\n",
        "\n",
        "This means that the solution will still be a line, be it in a transformed and typically higher-dimensional space instead of in the original feature space. This also means that we obtain the least-squares solutions just as we did earlier on.\n",
        "\n",
        "With different feature expansions, we can strongly improve the performance of linear regression by taking into account interaction effects, quadratic or cubic effects... However, the risk of overfitting also increases!\n",
        "\n",
        "In scikit-learn, [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) can generate both polynomial features as well as interaction effects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19BbiabqoTub"
      },
      "source": [
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE:</b>\n",
        "<p> Using the documentation. Expand the previous analysis with Polynomial Features. You have to fit the polynomial feature object on the train data and transform it before feeding it to the training function of the model.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kApgzK2oTYe"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(degree = 2)\n",
        "X_train_poly = ... # ADD CODE\n",
        "X_test_poly = ... # ADD CODE\n",
        "\n",
        "print(\"Shape before polynomial feature expansion:\", X_train.shape)\n",
        "print(\"Shape after polynomial feature expansion:\", X_train_poly.shape)\n",
        "\n",
        "# PASTE THE SOLUTION TO PREVIOUS EXERCISE HERE\n",
        "\n",
        "# Model instantiation, training, eval on test set ..\n",
        "\n",
        "# ADDITIONALLY: add code to eval on train set here ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kBvrhqyqYUV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "y_pred = LinReg.predict(X_test_poly) # make predictions for the test data\n",
        "\n",
        "r2 = r2_score(y_test, y_pred) # compute the MAE\n",
        "print('Test set r2 score: {}'.format(r2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjCiZPngqfLt"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(y_test,y_pred, s = 5, alpha = 0.3);\n",
        "ax.set_xlabel('y_test (Actual)');\n",
        "ax.set_ylabel('y_pred (Predicted)');\n",
        "ax.plot(np.arange(y_test.min()-1e5, y_test.max()+1e5), np.arange(y_test.min()-1e5, y_test.max()+1e5), color='orange');\n",
        "plt.xlim([y_test.min()-1e5, y_test.max()+1e5]);\n",
        "plt.ylim([y_test.min()-1e5, y_test.max()+1e5]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBBufmP0qufl"
      },
      "source": [
        "Our model should have successfully improved. Notice that we have chosen a polynomial degree of 2. In effect, this is a choice we have made, which we could additionally tune.\n",
        "In machine learning, we make a distinction between parameters (things learned by optimization of model parameters) and **hyperparameters**. The polynomial degree we have chosen is an example of the latter. Typically, we can try to find the best hyperparameter through **tuning**. This is a process that, in its simplest form, just loops through a possible set of values. For each value, trains and evaluates a model, and then takes the best hyperparameter value as final.\n",
        "\n",
        "Let's find the best polynomial degree hyperparameter:\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "<b>EXERCISE:</b>\n",
        "<p> Plug in the code of the previous exercise in the code below. Make it so that the degree argument of PolynomialFeatures isn't fixed, but is instead looped over. What is the effect on train / test performance on the degree parameter? Does that make sense to you?</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2zgitkmrFV-"
      },
      "outputs": [],
      "source": [
        "for degree in [1, 2, 3, 4]:\n",
        "    # PLUG IN SOLUTION TO PREVIOUS EXERCISE\n",
        "    # MAKE DEGREE ARGUMENT DEPEND ON LOOP ITERATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJGPK9pGvNp7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
